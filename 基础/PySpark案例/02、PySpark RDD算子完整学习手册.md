# PySpark RDDç®—å­å®Œæ•´å­¦ä¹ æ‰‹å†Œ

## ğŸ“‹ ç›®å½•

1. æ•°æ®è¾“å…¥ä¸è¾“å‡ºæµç¨‹
2. æ•°æ®è½¬æ¢ç®—å­
3. æ•°æ®èšåˆç®—å­
4. æ•°æ®è¿‡æ»¤ä¸å»é‡
5. æ•°æ®æ’åº
6. æ•°æ®è¾“å‡ºæ–¹æ³•
7. ä»£ç ç¤ºä¾‹æ±‡æ€»

------

## æ•°æ®è¾“å…¥ä¸è¾“å‡ºæµç¨‹

### ğŸ“Š Sparkç¼–ç¨‹æµç¨‹



```
æ•°æ®è¾“å…¥ â†’ RDDè®¡ç®— â†’ æ•°æ®è¾“å‡º
```



#### æ•°æ®è¾“å…¥æ–¹æ³•ï¼š

- `sc.parallelize()` - ä»Pythonå¯¹è±¡åˆ›å»ºRDD
- `sc.textFile()` - ä»æ–‡ä»¶è¯»å–æ•°æ®

#### æ•°æ®è®¡ç®—ï¼š

- è¿”å›å€¼æ˜¯RDDçš„ç®—å­ï¼ˆå¦‚mapã€flatMapã€filterç­‰ï¼‰
- é€šè¿‡é“¾å¼è°ƒç”¨è¿›è¡Œè®¡ç®—

#### æ•°æ®è¾“å‡ºï¼š

- è¿”å›å€¼æ˜¯Pythonå¯¹è±¡ï¼ˆå¦‚collectã€reduceã€countç­‰ï¼‰
- è¾“å‡ºåˆ°æ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚saveAsTextFileï¼‰

------

## æ•°æ®è½¬æ¢ç®—å­

### 1. **mapç®—å­**

**åŠŸèƒ½**ï¼šå°†RDDçš„æ•°æ®é€æ¡å¤„ç†ï¼Œè¿”å›æ–°çš„RDD

**è¯­æ³•**ï¼š

```
rdd.map(func)
# func: (T) â†’ U  # æ¥å—ä¸€ä¸ªå‚æ•°ï¼Œè¿”å›ä»»æ„ç±»å‹
```

**ç‰¹ç‚¹**ï¼š

- æ”¯æŒé“¾å¼è°ƒç”¨
- å¤„ç†é€»è¾‘åŸºäºä¼ å…¥çš„å‡½æ•°

**ç¤ºä¾‹**ï¼š

```
rdd.map(lambda x: x * 10).map(lambda x: x + 5)
```



### 2. **flatMapç®—å­**

**åŠŸèƒ½**ï¼šå¯¹RDDæ‰§è¡Œmapæ“ä½œï¼Œç„¶åè¿›è¡Œè§£é™¤åµŒå¥—æ“ä½œ

**è¯­æ³•**ï¼š

```
rdd.flatMap(func)
```

**ç‰¹ç‚¹**ï¼š

- æ¯”mapå¤šå‡ºè§£é™¤ä¸€å±‚åµŒå¥—çš„åŠŸèƒ½
- å¸¸ç”¨äºæ‹†åˆ†å­—ç¬¦ä¸²æˆ–åµŒå¥—åˆ—è¡¨

**ç¤ºä¾‹**ï¼š

```
# è¾“å…¥ï¼š["a b c", "a c e", "e c a"]
# è¾“å‡ºï¼š["a", "b", "c", "a", "c", "e", "e", "c", "a"]
rdd.flatMap(lambda x: x.split(" "))
```



------

## æ•°æ®èšåˆç®—å­

### 3. **reduceç®—å­**

**åŠŸèƒ½**ï¼šå¯¹RDDæ•°æ®é›†æŒ‰ç…§ä¼ å…¥çš„é€»è¾‘è¿›è¡Œèšåˆ

**è¯­æ³•**ï¼š

```
rdd.reduce(func)
# func: (T, T) â†’ T  # æ¥å—2ä¸ªå‚æ•°ï¼Œè¿”å›1ä¸ªå€¼ï¼Œç±»å‹ä¸€è‡´
```



**ç¤ºä¾‹**ï¼š

```
rdd.reduce(lambda a, b: a + b)  # æ±‚å’Œ
```



### 4. **reduceByKeyç®—å­**

**åŠŸèƒ½**ï¼šé’ˆå¯¹KVå‹RDDï¼Œè‡ªåŠ¨æŒ‰ç…§keyåˆ†ç»„ï¼Œå®Œæˆç»„å†…æ•°æ®çš„èšåˆ

**è¯­æ³•**ï¼š

```
rdd.reduceByKey(func)
# func: (v, v) â†’ v  # æ¥å—2ä¸ªç›¸åŒç±»å‹çš„å‚æ•°ï¼Œè¿”å›ç›¸åŒç±»å‹
```



**ç‰¹ç‚¹**ï¼š

- è‡ªåŠ¨æŒ‰ç…§keyåˆ†ç»„
- å‡½æ•°åªè´Ÿè´£èšåˆï¼Œä¸ç†ä¼šåˆ†ç»„
- é€‚ç”¨äºè¯é¢‘ç»Ÿè®¡ç­‰åœºæ™¯

**ç¤ºä¾‹**ï¼š

```
# è¾“å…¥ï¼š[('a', 1), ('a', 1), ('b', 1), ('b', 1)]
# è¾“å‡ºï¼š[('b', 2), ('a', 2)]
rdd.reduceByKey(lambda a, b: a + b)
```



**èšåˆé€»è¾‘å›¾ç¤º**ï¼š

```
[1, 2, 3, 4, 5]
â†“ lambda a, b: a + b
1+2=3 â†’ 3+3=6 â†’ 6+4=10 â†’ 10+5=15
```



------

## æ•°æ®è¿‡æ»¤ä¸å»é‡

### 5. **filterç®—å­**

**åŠŸèƒ½**ï¼šè¿‡æ»¤æƒ³è¦çš„æ•°æ®è¿›è¡Œä¿ç•™

**è¯­æ³•**ï¼š

```
rdd.filter(func)
# func: (T) â†’ bool  # æ¥å—1ä¸ªå‚æ•°ï¼Œè¿”å›å¸ƒå°”å€¼
```



**è§„åˆ™**ï¼š

- è¿”å›Trueçš„æ•°æ®è¢«ä¿ç•™
- è¿”å›Falseçš„æ•°æ®è¢«ä¸¢å¼ƒ

**ç¤ºä¾‹**ï¼š

```
# ä¿ç•™å¥‡æ•°
rdd.filter(lambda x: x % 2 == 1)
```



### 6. **distinctç®—å­**

**åŠŸèƒ½**ï¼šå¯¹RDDæ•°æ®è¿›è¡Œå»é‡ï¼Œè¿”å›æ–°RDD

**è¯­æ³•**ï¼š

```
rdd.distinct()  # æ— éœ€ä¼ å‚
```



**ç¤ºä¾‹**ï¼š

```
# è¾“å…¥ï¼š[1, 1, 3, 3, 5, 5, 6, 6, 9, 9]
# è¾“å‡ºï¼š[1, 3, 5, 6, 9]
rdd.distinct()
```



------

## æ•°æ®æ’åº

### 7. **sortByç®—å­**

**åŠŸèƒ½**ï¼šå¯¹RDDæ•°æ®è¿›è¡Œæ’åºï¼ŒåŸºäºæŒ‡å®šçš„æ’åºä¾æ®

**è¯­æ³•**ï¼š

```
rdd.sortBy(func, ascending=False, numPartitions=1)
# func: (T) â†’ U: å‘ŠçŸ¥æŒ‰ç…§å“ªä¸ªæ•°æ®è¿›è¡Œæ’åº
# ascending: Trueå‡åºï¼ŒFalseé™åº
# numPartitions: ç”¨å¤šå°‘åˆ†åŒºæ’åºï¼ˆå…¨å±€æ’åºéœ€è®¾ä¸º1ï¼‰
```



**ç¤ºä¾‹**ï¼š

```
# æŒ‰å…ƒç»„çš„ç¬¬äºŒä¸ªå…ƒç´ é™åºæ’åº
result.sortBy(lambda x: x[1], ascending=False, numPartitions=1)
```



------

## æ•°æ®è¾“å‡ºæ–¹æ³•

### å°†RDDè½¬æ¢ä¸ºPythonå¯¹è±¡

#### 8. **collectç®—å­**

**åŠŸèƒ½**ï¼šå°†RDDå„ä¸ªåˆ†åŒºå†…çš„æ•°æ®ï¼Œç»Ÿä¸€æ”¶é›†åˆ°Driverä¸­ï¼Œå½¢æˆä¸€ä¸ªListå¯¹è±¡

**è¯­æ³•**ï¼š

```
rdd.collect()  # è¿”å›å€¼æ˜¯list
```



#### 9. **takeç®—å­**

**åŠŸèƒ½**ï¼šå–RDDçš„å‰Nä¸ªå…ƒç´ ï¼Œç»„åˆæˆlistè¿”å›

**è¯­æ³•**ï¼š

```
rdd.take(N)
```



**ç¤ºä¾‹**ï¼š

```
>>> sc.parallelize([3,2,1,4,5,6]).take(5)
[3, 2, 1, 4, 5]
```



#### 10. **countç®—å­**

**åŠŸèƒ½**ï¼šè®¡ç®—RDDæœ‰å¤šå°‘æ¡æ•°æ®ï¼Œè¿”å›å€¼æ˜¯ä¸€ä¸ªæ•°å­—

**è¯­æ³•**ï¼š

```
rdd.count()
```



**ç¤ºä¾‹**ï¼š

```
>>> sc.parallelize([3,2,1,4,5,6]).count()
6
```



### å°†RDDè¾“å‡ºåˆ°æ–‡ä»¶

#### 11. **saveAsTextFileç®—å­**

**åŠŸèƒ½**ï¼šå°†RDDçš„æ•°æ®å†™å…¥æ–‡æœ¬æ–‡ä»¶ä¸­

**è¯­æ³•**ï¼š

```
rdd.saveAsTextFile("è·¯å¾„")
```



**ç‰¹ç‚¹**ï¼š

- æ”¯æŒæœ¬åœ°å†™å‡ºã€HDFSç­‰æ–‡ä»¶ç³»ç»Ÿ
- è¾“å‡ºç»“æœæ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹
- æœ‰å‡ ä¸ªåˆ†åŒºå°±è¾“å‡ºå¤šå°‘ä¸ªç»“æœæ–‡ä»¶

------

## åˆ†åŒºæ§åˆ¶

### ä¿®æ”¹RDDåˆ†åŒºæ•°ä¸º1çš„æ–¹æ³•

#### æ–¹æ³•1ï¼šSparkConfå¯¹è±¡è®¾ç½®å…¨å±€å¹¶è¡Œåº¦

```
conf = SparkConf().setMaster("local[*]").setAppName("test_spark")
conf.set("spark.default.parallelism", "1")  # è®¾ç½®ä¸º1ä¸ªåˆ†åŒº
sc = SparkContext(conf=conf)
```



#### æ–¹æ³•2ï¼šåˆ›å»ºRDDæ—¶è®¾ç½®åˆ†åŒºæ•°

```
rdd1 = sc.parallelize([1, 2, 3, 4, 5], numSlices=1)
# ç®€å†™å½¢å¼
rdd1 = sc.parallelize([1, 2, 3, 4, 5], 1)
```



------

## ä»£ç ç¤ºä¾‹æ±‡æ€»

### å®Œæ•´WordCountç¤ºä¾‹ï¼ˆå¸¦æ’åºï¼‰

```
from pyspark import SparkConf, SparkContext
import os

os.environ["PYSPARK_PYTHON"] = r"pythonè§£é‡Šå™¨è·¯å¾„"
conf = SparkConf().setMaster("local[*]").setAppName("test_spark")
sc = SparkContext(conf=conf)

# 1. è¯»å–æ•°æ®æ–‡ä»¶
rdd = sc.textFile("hello.txt")

# 2. å–å‡ºå…¨éƒ¨å•è¯
word_rdd = rdd.flatMap(lambda x: x.split(" "))

# 3. è½¬æ¢ä¸ºäºŒå…ƒå…ƒç»„
word_with_one_rdd = word_rdd.map(lambda x: (x, 1))

# 4. åˆ†ç»„å¹¶æ±‚å’Œ
result = word_with_one_rdd.reduceByKey(lambda x, y: x + y)

# 5. æŒ‰è¯é¢‘é™åºæ’åº
sorted_result = result.sortBy(lambda x: x[1], ascending=False, numPartitions=1)

print(sorted_result.collect())
sc.stop()
```



### æ•°æ®è¾“å‡ºç»¼åˆç¤ºä¾‹

```
from pyspark import SparkConf, SparkContext
import os

os.environ["PYSPARK_PYTHON"] = r"pythonè§£é‡Šå™¨è·¯å¾„"
conf = SparkConf().setMaster("local[*]").setAppName("test_spark")
sc = SparkContext(conf=conf)

# å‡†å¤‡RDD
rdd = sc.parallelize([1, 2, 3, 4, 5])

# collectç®—å­
rdd_list = rdd.collect()
print(f"collectç»“æœ: {rdd_list}, ç±»å‹: {type(rdd_list)}")

# reduceç®—å­
num = rdd.reduce(lambda x, y: x + y)
print(f"reduceæ±‚å’Œç»“æœ: {num}")

# takeç®—å­
take_list = rdd.take(3)
print(f"takeå‰3ä¸ª: {take_list}")

# countç®—å­
num_count = rdd.count()
print(f"å…ƒç´ ä¸ªæ•°: {num_count}")
```



### æ–‡ä»¶è¾“å‡ºç¤ºä¾‹

```
from pyspark import SparkConf, SparkContext
import os

os.environ["PYSPARK_PYTHON"] = r"pythonè§£é‡Šå™¨è·¯å¾„"
os.environ["HADOOP_HOME"] = 'hadoopè·¯å¾„'
conf = SparkConf().setMaster("local[*]").setAppName("test_spark")
sc = SparkContext(conf=conf)

# åˆ›å»ºRDDï¼ˆè®¾ç½®ä¸º1ä¸ªåˆ†åŒºï¼‰
rdd1 = sc.parallelize([1, 2, 3, 4, 5], 1)
rdd2 = sc.parallelize([("Hello", 3), ("Spark", 5), ("Hi", 7)], 1)
rdd3 = sc.parallelize([[1, 3, 5], [6, 7, 9], [11, 13, 11]], 1)

# è¾“å‡ºåˆ°æ–‡ä»¶
rdd1.saveAsTextFile("output1")
rdd2.saveAsTextFile("output2")
rdd3.saveAsTextFile("output3")
```



------

## ğŸ¯ å…³é”®è¦ç‚¹æ€»ç»“

| ç®—å­ç±»å‹ | ç®—å­åç§°       | ä¸»è¦åŠŸèƒ½       | è¿”å›å€¼ç±»å‹ |
| :------- | :------------- | :------------- | :--------- |
| è½¬æ¢ç®—å­ | map            | é€æ¡å¤„ç†æ•°æ®   | RDD        |
| è½¬æ¢ç®—å­ | flatMap        | å¤„ç†å¹¶è§£é™¤åµŒå¥— | RDD        |
| èšåˆç®—å­ | reduce         | æ•°æ®èšåˆ       | Pythonå€¼   |
| èšåˆç®—å­ | reduceByKey    | æŒ‰keyåˆ†ç»„èšåˆ  | RDD        |
| è¿‡æ»¤ç®—å­ | filter         | æ¡ä»¶è¿‡æ»¤       | RDD        |
| å»é‡ç®—å­ | distinct       | æ•°æ®å»é‡       | RDD        |
| æ’åºç®—å­ | sortBy         | æ•°æ®æ’åº       | RDD        |
| è¾“å‡ºç®—å­ | collect        | æ”¶é›†ä¸ºåˆ—è¡¨     | List       |
| è¾“å‡ºç®—å­ | take           | å–å‰Nä¸ª        | List       |
| è¾“å‡ºç®—å­ | count          | ç»Ÿè®¡æ•°é‡       | Int        |
| è¾“å‡ºç®—å­ | saveAsTextFile | è¾“å‡ºåˆ°æ–‡ä»¶     | æ—          |

### æ³¨æ„äº‹é¡¹ï¼š

1. **é“¾å¼è°ƒç”¨**ï¼šå¯¹äºè¿”å›å€¼æ˜¯æ–°RDDçš„ç®—å­ï¼Œå¯ä»¥é€šè¿‡é“¾å¼è°ƒç”¨å¤šæ¬¡å¤„ç†
2. **å…¨å±€æ’åº**ï¼šä½¿ç”¨sortByè¿›è¡Œå…¨å±€æ’åºæ—¶éœ€è¦è®¾ç½®`numPartitions=1`
3. **åˆ†åŒºæ§åˆ¶**ï¼šå¯ä»¥é€šè¿‡é…ç½®æˆ–å‚æ•°æ§åˆ¶RDDçš„åˆ†åŒºæ•°
4. **ç¯å¢ƒé…ç½®**ï¼šéœ€è¦æ­£ç¡®é…ç½®Pythonå’ŒHadoopç¯å¢ƒè·¯å¾„
5. **æ•°æ®å€¾æ–œ**ï¼šreduceByKeyå¯èƒ½é‡åˆ°æ•°æ®å€¾æ–œé—®é¢˜ï¼Œéœ€è¦æ³¨æ„ä¼˜åŒ–